{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q-learning for the toy problem taxi #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "\n",
    "Rendering:\n",
    "\n",
    "* blue: passenger\n",
    "\n",
    "* magenta: destination\n",
    "\n",
    "* yellow: empty taxi\n",
    "\n",
    "* green: full taxi\n",
    "\n",
    "* other letters: locations\n",
    "\n",
    "https://gym.openai.com/envs/Taxi-v2/\n",
    "\n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create taxi environment ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show an initial state ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Q table and initialize it ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 500\n",
      "Number of actions: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of states:\", state_size)\n",
    "print(\"Number of actions:\", action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((state_size, action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 100 * 1000   # total episodes\n",
    "total_test_episodes = 1000    # total test episodes\n",
    "max_steps = 99                # max steps per episode\n",
    "\n",
    "learning_rate = 0.1           # learning rate\n",
    "gamma = 0.95                  # discounting factor\n",
    "\n",
    "# Parameters for epsilon greedy strategy\n",
    "epsilon = 1.0                 # Exploration probability\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.0             # Minimum exploration probability \n",
    "decay_rate = 0.0001           # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning algorithm ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode      0, total reward -378.0, epsilon 1.000000\n",
      "Episode  10000, total reward  -33.0, epsilon 0.367916\n",
      "Episode  20000, total reward  -25.0, epsilon 0.135349\n",
      "Episode  30000, total reward    3.0, epsilon 0.049792\n",
      "Episode  40000, total reward    9.0, epsilon 0.018317\n",
      "Episode  50000, total reward   15.0, epsilon 0.006739\n",
      "Episode  60000, total reward    8.0, epsilon 0.002479\n",
      "Episode  70000, total reward    9.0, epsilon 0.000912\n",
      "Episode  80000, total reward   10.0, epsilon 0.000335\n",
      "Episode  90000, total reward   12.0, epsilon 0.000123\n"
     ]
    }
   ],
   "source": [
    "episode_reward_list = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    # reset the environment\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # epsilon greedy strategy \n",
    "        if random.uniform(0.0, 1.0) <= epsilon:\n",
    "            # random choice\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # select best action in state s\n",
    "            # a = argmax_a' q(s,a')\n",
    "            action = np.argmax(q_table[state,:])\n",
    "\n",
    "        # take action a, get reward r, and observe next_state s'\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # update q table according to transition (s, a, r, s')\n",
    "        # q(s,a) := q(s,a) + alpha [r + gamma * max q(s',a') - q(s,a)]\n",
    "        q_table[state, action] += learning_rate * (reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action])\n",
    "        \n",
    "        total_reward += reward\n",
    "                \n",
    "        # If done, then finish episode, else update state\n",
    "        if done: \n",
    "            break\n",
    "        else:\n",
    "            state = new_state\n",
    "\n",
    "    if episode % (10 * 1000) == 0: \n",
    "        print(\"Episode {0:6}, total reward {1:6.1f}, epsilon {2:.6f}\".format(episode, total_reward, epsilon))\n",
    "            \n",
    "    # decrease epsilon \n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode) \n",
    "    episode_reward_list.append((episode, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize learning progress ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, rews = np.array(episode_reward_list).T\n",
    "smoothed_rews = running_mean(rews, 5)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe trained agent ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        # take the best action\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        #env.render()\n",
    "        \n",
    "        if done:\n",
    "            reward_list.append(total_reward)\n",
    "            break\n",
    "            \n",
    "        state = new_state        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_averages = np.convolve(reward_list, np.ones(100,) / 100, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(running_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
