{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run one episode \n",
    "def run_episode(environment, policy = None, render = False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_index = 0\n",
    "    for i in range(1000):\n",
    "        if render:\n",
    "            environment.render()\n",
    "        if policy is None:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            a,b = observationState(environment, obs)\n",
    "            action = policy[a][b]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += gamma ** step_index * reward\n",
    "        step_index += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observationState(environment, obs):\n",
    "    env_low = environment.observation_space.low\n",
    "    env_high = environment.observation_space.high\n",
    "    env_dx = (env_high - env_low) / n_states\n",
    "    a = int((obs[0] - env_low[0])/env_dx[0])\n",
    "    b = int((obs[1] - env_low[1])/env_dx[1])\n",
    "    return a, b\n",
    "\n",
    "def discretize(obs, env):\n",
    "    bucket = [1, 1, 6, 12]\n",
    "    upper_bound = env.observation_space.high\n",
    "    lower_bound = env.observation_space.low\n",
    "    upper_bound[1] = 0.5\n",
    "    lower_bound[1] = -0.5\n",
    "    upper_bound[3] = math.radians(50)\n",
    "    lower_bound[3] = -math.radians(50)\n",
    "    newobs = []\n",
    "    for i in range(len(upper_bound)):\n",
    "        ratio = (obs[i] + abs(lower_bound[i])) / (upper_bound[i] - lower_bound[i])\n",
    "        temp = int(round((bucket[i] - 1) * ratio))\n",
    "        temp = min(bucket[i] - 1, max(0, temp))\n",
    "        newobs.append(temp)\n",
    "\n",
    "    return np.array(newobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a and p must have same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c579acf647c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mlogits_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_exp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscretize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a and p must have same size"
     ]
    }
   ],
   "source": [
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "n_states = 46\n",
    "initial_lr = 1.0 # Learning rate\n",
    "min_lr = 0.003\n",
    "gamma = 1.0\n",
    "eps = 0.02\n",
    "qTable = np.zeros((n_states, n_states, 3))\n",
    "for i in range(10000):\n",
    "    obs = env.reset()\n",
    "    obs = discretize(obs, env)\n",
    "    total_reward = 0\n",
    "    eta = max(min_lr, initial_lr * (0.85 ** (i//100)))\n",
    "    for j in range(10000):\n",
    "            a, b = observationState(env, obs)\n",
    "            if np.random.uniform(0, 1) < eps:\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                logits = qTable[a][b]\n",
    "                logits_exp = np.exp(logits)\n",
    "                probs = logits_exp / np.sum(logits_exp)\n",
    "                action = np.random.choice(env.action_space.n, p=probs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            obs = discretize(obs, env)\n",
    "            total_reward += reward\n",
    "            # update q table\n",
    "            a_, b_ = observationState(env, obs)\n",
    "            qTable[a][b][action] = qTable[a][b][action] + eta * (reward + gamma *  np.max(qTable[a_][b_]) - qTable[a][b][action])\n",
    "            if done:\n",
    "                break\n",
    "    if i % 100 == 0:\n",
    "        print('Iteration #%d -- Total reward = %d.' %(i+1, total_reward))\n",
    "solution_policy = np.argmax(qTable, axis=2)\n",
    "solution_policy_scores = [run_episode(env, solution_policy, False) for _ in range(100)]\n",
    "print(\"Average score of solution = \", np.mean(solution_policy_scores))\n",
    "# Animate it\n",
    "run_episode(env, solution_policy, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1 -- Total reward = 11.\n",
      "Iteration #101 -- Total reward = 9.\n",
      "Iteration #201 -- Total reward = 17.\n",
      "Iteration #301 -- Total reward = 9.\n",
      "Iteration #401 -- Total reward = 10.\n",
      "Iteration #501 -- Total reward = 9.\n",
      "Iteration #601 -- Total reward = 12.\n",
      "Iteration #701 -- Total reward = 10.\n",
      "Iteration #801 -- Total reward = 9.\n",
      "Iteration #901 -- Total reward = 11.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -32880 is out of bounds for axis 0 with size 30000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9b393672d6db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration #%d -- Total reward = %d.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0msolution_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqTable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0msolution_policy_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolution_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average score of solution = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolution_policy_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Animate it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9b393672d6db>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(environment, policy, render)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservationState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstep_index\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -32880 is out of bounds for axis 0 with size 30000"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import time\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# run one episode\n",
    "def run_episode(environment, policy = None, render = False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_index = 0\n",
    "    for i in range(1000):\n",
    "        if render:\n",
    "            environment.render()\n",
    "        if policy is None:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            a,b = observationState(environment, obs)\n",
    "            action = policy[a][b]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += gamma ** step_index * reward\n",
    "        step_index += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def observationState(environment, obs):\n",
    "    env_low = environment.observation_space.low\n",
    "    env_high = environment.observation_space.high\n",
    "    env_dx = (env_high - env_low) / n_states\n",
    "    a = int((obs[0] - env_low[0])/env_dx[0])\n",
    "    b = int((obs[1] - env_low[1])/env_dx[1])\n",
    "    return a, b\n",
    "\n",
    "def discretize(obs, env):\n",
    "    bucket = [1, 1, 6, 12]\n",
    "    upper_bound = env.observation_space.high\n",
    "    lower_bound = env.observation_space.low\n",
    "    upper_bound[1] = 0.5\n",
    "    lower_bound[1] = -0.5\n",
    "    upper_bound[3] = math.radians(50)\n",
    "    lower_bound[3] = -math.radians(50)\n",
    "    newobs = []\n",
    "    for i in range(len(upper_bound)):\n",
    "        ratio = (obs[i] + abs(lower_bound[i])) / (upper_bound[i] - lower_bound[i])\n",
    "        temp = int(round((bucket[i] - 1) * ratio))\n",
    "        temp = min(bucket[i] - 1, max(0, temp))\n",
    "        newobs.append(temp)\n",
    "\n",
    "    return np.array(newobs)\n",
    "\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "n_states = 30000\n",
    "initial_lr = 0.09 # Learning rate\n",
    "min_lr = 0.003\n",
    "gamma = 1.0\n",
    "eps = 0.1\n",
    "qTable = np.zeros((n_states, n_states, 2))\n",
    "for i in range(1000):\n",
    "    obs = env.reset()\n",
    "    obs = discretize(obs, env)\n",
    "    total_reward = 0\n",
    "    eta = max(min_lr, initial_lr * (0.85 ** (i//100)))\n",
    "    for j in range(1000):\n",
    "            a, b = observationState(env, obs)\n",
    "            if np.random.uniform(0, 1) < eps:\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                logits = qTable[a][b]\n",
    "                logits_exp = np.exp(logits)\n",
    "                probs = logits_exp / np.sum(logits_exp)\n",
    "                action = np.random.choice(env.action_space.n, p=probs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            obs = discretize(obs, env)\n",
    "            total_reward += reward\n",
    "            # update q table\n",
    "            a_, b_ = observationState(env, obs)\n",
    "            qTable[a][b][action] = qTable[a][b][action] + eta * (reward + gamma *  np.max(qTable[a_][b_]) - qTable[a][b][action])\n",
    "            if done:\n",
    "                break\n",
    "    if i % 100 == 0:\n",
    "        print('Iteration #%d -- Total reward = %d.' %(i+1, total_reward))\n",
    "solution_policy = np.argmax(qTable, axis=2)\n",
    "solution_policy_scores = [run_episode(env, solution_policy, False) for _ in range(100)]\n",
    "print(\"Average score of solution = \", np.mean(solution_policy_scores))\n",
    "# Animate it\n",
    "run_episode(env, solution_policy, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 19.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 61.12 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 162.81 ticks.\n",
      "('Ran 260 episodes. Solved after 160 trials :', ' score :', 200.22)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "class QCartPoleSolver():\n",
    "    def __init__(self, buckets=(1, 1, 6, 12,), n_episodes=1000, n_win_ticks=200, min_alpha=0.1, min_epsilon=0.1, gamma=1.0, ada_divisor=25, max_env_steps=None, quiet=False, monitor=False):\n",
    "        self.buckets = buckets # down-scaling feature space to discrete range\n",
    "        self.n_episodes = n_episodes # training episodes \n",
    "        self.n_win_ticks = n_win_ticks # average ticks over 100 episodes required for win\n",
    "        self.min_alpha = min_alpha # learning rate\n",
    "        self.min_epsilon = min_epsilon # exploration rate\n",
    "        self.gamma = gamma # discount factor\n",
    "        self.ada_divisor = ada_divisor # only for development purposes\n",
    "        self.quiet = quiet\n",
    "\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "        #if monitor: self.env = gym.wrappers.Monitor(self.env, 'tmp/cartpole-1', force=True) # record results for upload\n",
    "\n",
    "        self.Q = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "\n",
    "    def discretize(self, obs):\n",
    "        upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50)]\n",
    "        lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50)]\n",
    "        ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "        new_obs = [int(round((self.buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "        new_obs = [min(self.buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "        return tuple(new_obs)\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.Q[state])\n",
    "\n",
    "    def update_q(self, state_old, action, reward, state_new, alpha):\n",
    "        self.Q[state_old][action] += alpha * (reward + self.gamma * np.max(self.Q[state_new]) - self.Q[state_old][action])\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.min_epsilon, min(1, 1.0 - ((t + 1) / self.ada_divisor)))\n",
    "        #return max(self.min_epsilon, 1)\n",
    "\n",
    "    def get_alpha(self, t):\n",
    "        return max(self.min_alpha, min(1.0, 1.0 - ((t + 1) / self.ada_divisor)))\n",
    "        #return max(self.min_alpha, 1)\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            current_state = self.discretize(self.env.reset())\n",
    "\n",
    "            alpha = self.get_alpha(e)\n",
    "            epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "            i = 1\n",
    "\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = self.choose_action(current_state, epsilon)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize(obs)\n",
    "                self.update_q(current_state, action, reward, new_state, alpha)\n",
    "                current_state = new_state\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials :'.format(e, e - 100), \" score :\", mean_score)\n",
    "                return e - 100\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "        if not self.quiet: print('Did not solve after {} episodes :'.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    solver = QCartPoleSolver()\n",
    "    solver.run()\n",
    "    # gym.upload('tmp/cartpole-1', api_key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
