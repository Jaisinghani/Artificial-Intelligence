{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breadth First Search: \n",
    "In BFS the search is performed by traversing a tree from the root node an exploring all its neighboring nodes. i.e. all the nodes at the current depth before moving to the next level in the tree. It can be implemented using FIFO queue data structure. It finds shortest path to the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth First Search: \n",
    "In DFS the search is performed by traversing a tree from the root node until all the child nodes are explored. It takes a node and continues to explore the child node of every node until there are no child nodes, if no child nodes found it backtracks to a new path. It can be implemented using LIFO stack data structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform Cost Search:\n",
    "UCS is used to find the shortest path in the weighted graph. The strategy in UCS is to expand the cheapest node first. It processes all the nodes with cost less than the cheapest solution. It can be implemented using priority queue data structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A* Search: \n",
    "The A* star is an informed search algorithm that finds the path based on a heuristic, such that the it traverses the least cost path to reach the goal. \n",
    "f(n) = g(n) +h(n)\n",
    "Where n is the node, g(n) is the cost of the path from start node to n, h(n) is the heuristic that estimates the least cost path to goal. The heuristic helps in calculating the cost to reach the goal state from a particular state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dijkstra’s Algorithm: \n",
    "This algorithm starts with a node considering it as origin and finds out the distance between the origin and every other node. All the nodes are marked as unvisited. Once the distances for all the node from the origin node are calculated the origin node is marked as visited and the node that has the shortest distance from the current origin node is now made the origin node.  A visited node will never be checked again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimax: \n",
    "It is a decision making algorithm and is used in two player turn based games. The algorithm’s goal is to find the optimal next move. The game is such that one player tries to maximize the score and the other player tries to minimize it. The algorithm assumes that both the players are playing optimally. It find the next optimal move the algorithm evaluates the potential opponent’s move.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Beta Pruning: \n",
    "Avoiding searching a part of a tree is called pruning. Alpha beta pruning is an adversarial search algorithm used for two player games. It tries to minimize the number of nodes been evaluated by minimax algorithm. Alpha is minimal score that player MAX is guaranteed to attain.Beta maximum score that player MAX can hope to obtain. So the nodes that have a value greater that alpha will not be explored if a node is found that has min value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectimax : \n",
    "In Minimax it is assumed that both the players make optimal decisions, in expectimax we assume that the opponent will make suboptimal decisions. The chance nodes will be the minimum nodes and The the expected utilities is the weighted average of all child values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov decision process: \n",
    "It is helps in decision making for situations where outcomes or the next action are partly in the control of the agent and partly random. It generates a policy for the agent based on the rewards and the probability of an action happening from a state. (S, As, Pa(s,s’), Ra) tuple is used in MDP where \n",
    "S = set of states\n",
    "As = finite set of actions that can be performed when in a state\n",
    "Pa(s,s’) = the probability that an action from state s will lead to s’\n",
    "Ra = reward received when action a is performed from s that leads to s’\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning: \n",
    "This algorithm works on rewards and penalties. The agent interacts with the environment and for every action taken by the agent it receives a reward or a punishment. Based on the multiple interactions with the environment and rewards and penalties received for every action the agent learns an optimal policy of actions to be performed to maximize the reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning:\n",
    "Q-learning is a values-based learning algorithm in reinforcement learning. It makes use of a Q Table where maximum expected future rewards for every action at a particular state is stored. It act as a lookup table for actions to be performed.\n",
    "\n",
    "Qnew(st,at) = (1-learningrate)Q(at,at) + learningRate * (reward+ gamma*max(Q(st+1, a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
