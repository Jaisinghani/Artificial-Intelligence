{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "numberOfStates = 40\n",
    "numberOfEpisodes = 10000\n",
    "numberOfItrs = 10000\n",
    "initLearningRate = 1.0\n",
    "minLearningRate = 0.003\n",
    "gamma = 1.0\n",
    "epsilon = 0.02\n",
    "qTable = np.zeros((numberOfStates, numberOfStates, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEpisode(environment, policy = None, render = False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_index = 0\n",
    "    for i in range(1000):\n",
    "        if render:\n",
    "            environment.render()\n",
    "        if policy is None:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            a,b = observationState(environment, obs)\n",
    "            action = policy[a][b]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += gamma ** step_index * reward\n",
    "        step_index += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observationState(environment, obs):\n",
    "    env_low = environment.observation_space.low\n",
    "    env_high = environment.observation_space.high\n",
    "    env_dx = (env_high - env_low) / numberOfStates\n",
    "    a = int((obs[0] - env_low[0])/env_dx[0])\n",
    "    b = int((obs[1] - env_low[1])/env_dx[1])\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('At episode :', 1, 'Total Reward is :', -200.0)\n",
      "('At episode :', 1001, 'Total Reward is :', -200.0)\n",
      "('At episode :', 2001, 'Total Reward is :', -200.0)\n",
      "('At episode :', 3001, 'Total Reward is :', -200.0)\n",
      "('At episode :', 4001, 'Total Reward is :', -200.0)\n",
      "('At episode :', 5001, 'Total Reward is :', -200.0)\n",
      "('At episode :', 6001, 'Total Reward is :', -200.0)\n",
      "('At episode :', 7001, 'Total Reward is :', -200.0)\n",
      "('At episode :', 8001, 'Total Reward is :', -198.0)\n",
      "('At episode :', 9001, 'Total Reward is :', -200.0)\n",
      "('Average score for episodes = ', -129.54390000000001)\n"
     ]
    }
   ],
   "source": [
    "for i in range(numberOfEpisodes):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    learningRate = max(minLearningRate, initLearningRate * (0.85 ** (i//100)))\n",
    "    for j in range(numberOfItrs):\n",
    "            a, b = observationState(env, obs)\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                x = qTable[a][b]\n",
    "                y = np.exp(x)\n",
    "                probs = y / np.sum(y)\n",
    "                action = np.random.choice(env.action_space.n, p=probs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            #env.render()\n",
    "            a_, b_ = observationState(env, obs)\n",
    "            qTable[a][b][action] = qTable[a][b][action] + learningRate * (reward + gamma *  np.max(qTable[a_][b_]) - qTable[a][b][action])\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "    if i % 1000 == 0:\n",
    "        print(\"At episode :\", i+1, \"Total Reward is :\", total_reward)\n",
    "policy = np.argmax(qTable, axis=2)\n",
    "policyScores = [runEpisode(env, policy, False) for _ in range(10000)]\n",
    "print(\"Average score for episodes = \", np.mean(policyScores))\n",
    "runEpisode(env, policy, True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
